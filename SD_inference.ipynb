{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install the diffusers library that encapsulates the model and the inference pipeline."
      ],
      "metadata": {
        "id": "Sfcgx3KJmErG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZPh19PxhNDN"
      },
      "outputs": [],
      "source": [
        "!pip install diffusers # state of the art library for diffusion model training and inference including text-to-image models\n",
        "!pip install imscore # library and collection of models fro aesthetic scoring\n",
        "!pip install einops # library needed for preparing the inputs for imscore models\n",
        "!pip install transformers==4.56.2 # state of the art library for language model training anf inference needed for diffusers library"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import the necessary libraries"
      ],
      "metadata": {
        "id": "XsxESOvYmyxc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from diffusers import UNet2DConditionModel, StableDiffusionPipeline, DPMSolverMultistepScheduler"
      ],
      "metadata": {
        "id": "BBnAGdY_hRum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the original model with the original UNet weights from Huggingface"
      ],
      "metadata": {
        "id": "8mZG2o7Hm6yD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the stable diffusion pipeline, autoecoder, textual encoder, and UNet\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\n",
        "        \"stabilityai/stable-diffusion-2-1\", torch_dtype=torch.float16, cache_dir=\"./cache\"\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "# noise scheduler according to which we will remove the predicted noise from the noisy latent\n",
        "pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "pipe = pipe.to('cuda')\n",
        "pipe.safety_checker = None\n",
        "\n",
        "# this is what defines the noisy latent from which the UNet will start from\n",
        "generator = torch.Generator(device='cuda')\n",
        "generator = generator.manual_seed(0)\n",
        "\n",
        "prompt = \"A fox in an autumn forest.\"\n",
        "# guidance scale is how much we want the prompt to guide the denoising process\n",
        "image_base = pipe(prompt=prompt, generator=generator, guidance_scale=5).images[0]\n",
        "image_base"
      ],
      "metadata": {
        "id": "Ula4g3aJm5SI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Instantiate the fine-tuned UNet with its new weights. Ensure that the UNet weights (.safetensors) and config.json are under sd-finetune/unet"
      ],
      "metadata": {
        "id": "Q0djDFsMpAcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_unet = UNet2DConditionModel.from_pretrained(\n",
        "                            \"KetevanK/SD_fine_tuning\",\n",
        "                            subfolder='unet',\n",
        "                            # access token since my model is in private repository\n",
        "                            token=\"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\",\n",
        "                            torch_dtype=torch.float16,\n",
        "                            cache_dir=\"./cache\").to('cuda')\n",
        "\n",
        "# Swapping base model UNet with my finetuned UNet\n",
        "pipe.unet = finetuned_unet\n",
        "# The generator seed should be fixed for both models. This ensures both UNets\n",
        "# start from the same noisy latent vector\n",
        "generator = torch.Generator(device='cuda')\n",
        "generator = generator.manual_seed(0)\n",
        "\n",
        "image_mine = pipe(prompt=prompt, generator=generator, guidance_scale=5).images[0]\n",
        "image_mine"
      ],
      "metadata": {
        "id": "UzmzeCEbhlzw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the generated images aesthetically with HPSv2 aesthetic model"
      ],
      "metadata": {
        "id": "VdzZhL27cpWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from einops import rearrange\n",
        "# imscore (https://github.com/RE-N-Y/imscore)\n",
        "# library offers a collection of aesthetic scoring models and was used in\n",
        "# my quantitative evaluation\n",
        "from imscore.hps.model import HPSv2\n",
        "from imscore.mps.model import MPS\n",
        "from imscore.pickscore.model import PickScorer\n",
        "from imscore.imreward.model import ImageReward\n",
        "# Since the aesthetic scorer is a fine-tuned CLIP model itself,\n",
        "# loading the model through the library will download the necessary files and\n",
        "# evaluate the images. Downloading the files might take a bit of time\n",
        "\n",
        "# Uncomment one of the below lines for aesthetic scoring\n",
        "scorer = HPSv2.from_pretrained(\"RE-N-Y/hpsv21\")\n",
        "# scorer = MPS.from_pretrained(\"RE-N-Y/mpsv1\")\n",
        "# scorer = ImageReward.from_pretrained(\"RE-N-Y/ImageReward\")\n",
        "# scorer = PickScorer(\"yuvalkirstain/PickScore_v1\")\n",
        "\n",
        "def convert_to_torch_tensor(pixels):\n",
        "\tpixels = np.array(pixels)\n",
        "\tpixels = rearrange(torch.tensor(pixels), \"h w c -> 1 c h w\") / 255.0\n",
        "\treturn pixels\n",
        "\n",
        "ims = [image_base, image_mine]\n",
        "image_tensor = [convert_to_torch_tensor(im) for im in ims]\n",
        "image_tensor = torch.cat(image_tensor, dim=0).to(\"cuda\")\n",
        "scorer.to(\"cuda\").eval()\n",
        "\n",
        "with torch.inference_mode():\n",
        "\tscores = scorer.score(image_tensor, [prompt]*len(ims))\n",
        "scores = scores.exp()/scores.exp().sum() # softmax the scores\n",
        "print(scores)\n",
        "\n",
        "# if scores[0] > scores[1] → image_base is better\n",
        "# if scores[0] < scores[1] → image_mine is better"
      ],
      "metadata": {
        "id": "N8YAT2OeTJSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wqeXsVMUeWGj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}